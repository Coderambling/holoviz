{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Using the `.plot` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\"> <strong>WORK IN PROGRESS:</strong> We are in the progress of updating these material in anticipation of a tutorial at the 2019 SciPy conference. Work will be complete by the morning of July 8th 2019. Check out <a href=\"https://github.com/pyviz/holoviz/tree/v0.1.1\">this tag</a> to access the materials as they were before these changes started. For the latest version of the tutorial, visit <a href=\"https://holoviz.org/tutorial\">holoviz.org</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have tried to visualize `pandas.DataFrame`s before, then you have likely encountered the `.plot` API. `.plot` provides a quick way to get from data to a visualization while keeping the focus on the data. Since this has become such a popular method of generating plots, other libraries have adopted the same interface. It is now possible to generate [matplotlib](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html), [plotly](https://github.com/santosjorge/cufflinks), [bokeh](https://github.com/PatrikHlobil/Pandas-Bokeh), [holoviews](https://hvplot.pyviz.org), or [vega](https://altair-viz.github.io/pdvega) plots all using a very similar approach. From the user's perspective, this has made the barrier to switching plotting library much lower. \n",
    "\n",
    "In this notebook we'll explore what is possible with the default `.plot` API and demonstrate the capabilities of `.hvplot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('../data/earthquakes.parq')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using default `.plot`\n",
    "\n",
    "The first thing that we'd like to do with this data is visualize the locations of every earthquake. So we would like to make a `scatter` plot where `x='longitude'` and `y='latitude'`. If you are familiar with the `pandas.plot` API you might expect to execute: `df.plot.scatter(x='longitude', y='latitude')`. Feel free to try this out in a new cell. It throws an error: `\n",
    "AttributeError: 'DataFrame' object has no attribute 'plot'`. Since we have a dask dataframe rather than a pandas dataframe, we need to first convert it to pandas. In order to make the data more manageable, we'll use just a fraction (1%) of it and call that `small_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = df.sample(frac=.01).compute()\n",
    "small_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a smaller dataset with just 21k earthquakes. We can use that to test out our visualizations before ramping back up to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.plot.scatter(x='longitude', y='latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.hvplot`\n",
    "That is a good place to start and you can start to see the structure of the edges of the plates (which often correspond with the edges of the continents). You can make a very similar plot with the same arguments using hvplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.hvplot.scatter(x='longitude', y='latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that many of the dots in the scatter that we've just created lie on top of one another. This is called \"overplotting\" and can be avoided in a variety of ways, such as by making the dots slightly transparent, or binning the data. These approaches have the downside of introducing bias because you need to choose the alpha or the edges of the bins, and in order to do that, you make assumptions about the data. Rather than use these traditional approaches, we use a library called [datashader](datashader.org) which aggregates to the pixel. In `hvplot` we can activate this capability by setting `datashade=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Try changing the alpha (try .1) on the plot above to see the effect of this approach, or create a hexbin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.hvplot.scatter(x='longitude', y='latitude', datashade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we are still only plotting 1% of the data (21k earthquakes). With datashader, we can easily plot all the original full dataset. Despite the fact that the full data are still in a dask object, we can still use the same API because `hvplot` differs from the other `.plot` libraries in that it doesn't just target pandas objects. Instead hvplot can be used with: \n",
    " - Pandas : DataFrame, Series (columnar/tabular data)\n",
    " - xarray : Dataset, DataArray (labelled multidimensional arrays)\n",
    " - Dask : DataFrame, Series (distributed/out of core arrays and columnar data)\n",
    " - Streamz : DataFrame(s), Series(s) (streaming columnar data)\n",
    " - Intake : DataSource (data catalogues)\n",
    " - GeoPandas : GeoDataFrame (geometry data)\n",
    " - NetworkX : Graph (network graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hvplot.scatter(x='longitude', y='latitude', datashade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** That probably felt very slow to render, and when you zoom into the plot, the image re-renders. This can also be slow (tens of seconds) when the data is being read from disk. If your computer has sufficient RAM, persist the dataset in memory to significantly speed up the time that it takes to re-render the plot on zoom events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hvplot.scatter(x='longitude', y='latitude', datashade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: If you are brave and don't mind restarting your kernel if it dies, create a scatter without setting datashade=True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Plots\n",
    "\n",
    "Next we'll look at the frequency of different magnitude earthquakes.\n",
    "\n",
    "| Magnitude     | Earthquake Effect | Estimated Number Each Year |\n",
    "|---------------|-------------------|----------------------------|\n",
    "| 2.5 or less   | Usually not felt, but can be recorded by seismograph. |900,000|\n",
    "| 2.5 to 5.4    | Often felt, but only causes minor damage. |30,000 |\n",
    "| 5.5 to 6.0    | Slight damage to buildings and other structures. |500 |\n",
    "| 6.1 to 6.9    | May cause a lot of damage in very populated areas. | 100 |\n",
    "| 7.0 to 7.9    | Major earthquake. Serious damage. | 20 |\n",
    "| 8.0 or greater| Great earthquake. Can totally destroy communities near the epicenter. | One every 5 to 10 years |\n",
    "\n",
    "As a first pass, we'll use a histogram first with `plot.hist` on the small data, then with `.hvplot.hist` on the full dataset. Before plotting we can clean the data by setting any magnitudes that are less than 0 to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df.copy()\n",
    "cleaned_df['mag'] = df.mag.where(df.mag > 0)\n",
    "cleaned_small_df = cleaned_df.sample(frac=.01).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_small_df.plot.hist(y='mag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can create a histogram of the whole dataset using hvplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.hvplot.hist(y='mag', bin_range=(0,10), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Create a kernel density estimate (kde) plot of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "\n",
    "Next we'll categorize the earthquakes based on depth. You can read about all the different variables available in this dataset [here](https://earthquake.usgs.gov/data/comcat/data-eventterms.php). In the interest of time, we'll use the small dataset and assume that it is representative of all the earthquakes. According to the [USGS page on earthquakes depths](https://earthquake.usgs.gov/learn/topics/determining_depth.php):\n",
    "\n",
    "> Shallow earthquakes are between 0 and 70 km deep; \n",
    "intermediate earthquakes, 70 - 300 km deep; and deep earthquakes, \n",
    "300 - 700 km deep. \n",
    "In general, the term \"deep-focus earthquakes\" is applied to earthquakes deeper than 70 km. \n",
    "All earthquakes deeper than 70 km are localized within great slabs of lithosphere that are sinking into the Earth's mantle.\n",
    "\n",
    "First we'll use `pd.cut` to split the small_dataset into depth categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_bins = [-np.inf, 70, 300, np.inf]\n",
    "depth_names = ['Shallow', 'Intermediate', 'Deep']\n",
    "depth_class_column = pd.cut(cleaned_small_df['depth'], depth_bins, labels=depth_names)\n",
    "\n",
    "cleaned_small_df.insert(1, 'depth_class', depth_class_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this new categorical variable to group our data, first we will overlay all our groups on the same plot using the `by` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_small_df.hvplot.hist(y='mag', by='depth_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Click on the legend to turn off certain categories and see what is behind them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Add subplots=True and width=300 to see the different classes side-by-side. The y-axis will be linked, so try zooming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instead use a widget to toggle between classes, use the `groupby` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_small_df.hvplot.bivariate(x='mag', y='depth', groupby='depth_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying by magnitude\n",
    "\n",
    "In addition to classifying by depth, we can classify by magnitude.\n",
    "\n",
    "| Class    | Magnitude | \n",
    "|----------|-----------|\n",
    "| Great    | 8 or more | \n",
    "| Major    | 7 - 7.9   | \n",
    "| Strong   | 6 - 6.9   |\n",
    "| Moderate | 5 - 5.9   |\n",
    "| Light    | 4 - 4.9   |\n",
    "| Minor    | 3 -3.9    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df = df[df.mag >= 3].compute()\n",
    "\n",
    "depth_class_column = pd.cut(classified_df['depth'], depth_bins, labels=depth_names)\n",
    "classified_df.insert(1, 'depth_class', depth_class_column)\n",
    "\n",
    "mag_bins = [2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 10]\n",
    "mag_names = ['Minor', 'Light', 'Moderate', 'Strong', 'Major', 'Great']\n",
    "mag_class_column = pd.cut(classified_df['mag'], mag_bins, labels=mag_names)\n",
    "classified_df.insert(11, 'mag_class', mag_class_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df.hvplot.heatmap(x='mag_class', y='depth_class', C='id', reduce_function=np.count_nonzero)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
